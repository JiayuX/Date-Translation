{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we build a LSTM-based seq2seq model with attention mechanism to translate human-readable date to machine-readable date. Seq2seq is a general category of machine learning models that take a sequence as input and output another sequence. A seq2seq model is composed of an encoder to encode the input sequence and a decoder to generate the output sequence based on the encoded information from the input sequence. The performance of a vanilla LSTM(/GRU/RNN)-based seq2seq model is limited by the amount of information that can be carried from the encoder to the decoder through the encoder final hidden state. To overcome this bottleneck, attention mechanism is invented by researchers, which allows the decoder to utilize the information of all the hidden states of the encoder at every step of generating a new element during the decoding stage. In that way, the model learns to pay attention to different parts of the input sequence when generating different parts of the output sequence, which results in better performance. This concept of attention was further improved later giving rise to the next generation NLP models, i.e. transformer, BERT and GPT, which are based upon multi-head self-attention. Seq2seq models can handle many NLP tasks such as language translation, conversational chatbot, text summarization and question answering, etc. Here, we build a date translator to automate the process of translating human-readable date to machine-readable date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsML57Id8g4J"
   },
   "source": [
    "## Data preparation & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1640667650046,
     "user": {
      "displayName": "Dennis X",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07673593424094128995"
     },
     "user_tz": 300
    },
    "id": "Mb-BmSXK0od4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import random\n",
    "from faker import Faker\n",
    "from babel.dates import format_date\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset needs to be formatted as\n",
    "```\n",
    "train_raw = [\n",
    "             [\"6/1/05\", \"2005-06-01\"],\n",
    "             [\"Sep 6, 2019\", \"2019-09-06\"],\n",
    "             [\"December 9, 1974\", \"1974-12-09\"],\n",
    "             [\"28 December, 1979\", \"1979-12-28\"],\n",
    "             [\"14 Dec 2001\", \"2001-12-14\"]\n",
    "            ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "\n",
    "Faker.seed(101)\n",
    "random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FORMATS = ['medium', # MMM d, YYY\n",
    "           'long', # MMMM dd, YYY\n",
    "           'd MMM, YYY', \n",
    "           'dd MMMM, YYY',\n",
    "           'YYY, MMM d',\n",
    "           'YYY, MMMM dd',\n",
    "           'YYY, d MMM', \n",
    "           'YYY, dd MMMM',\n",
    "           'dd/MM/YYY',\n",
    "           'd/MM/YYY',\n",
    "           'YYYY/MM/dd',\n",
    "           'YYYY/MM/d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jul 19, 2009 -> 2009-07-19\n",
      "April 3, 1983 -> 1983-04-03\n",
      "11 Sep, 2006 -> 2006-09-11\n",
      "29 May, 1994 -> 1994-05-29\n",
      "2001, Oct 15 -> 2001-10-15\n",
      "1973, April 20 -> 1973-04-20\n",
      "2015, 24 Feb -> 2015-02-24\n",
      "2004, 07 April -> 2004-04-07\n",
      "06/08/1984 -> 1984-08-06\n",
      "20/12/2010 -> 2010-12-20\n",
      "1985/02/01 -> 1985-02-01\n",
      "1989/08/20 -> 1989-08-20\n"
     ]
    }
   ],
   "source": [
    "for form in FORMATS:\n",
    "    date = fake.date_object()\n",
    "    print(f\"{format_date(date, format=form, locale='en')} -> {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_date():\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    try:\n",
    "        date = format_date(dt, format = random.choice(FORMATS), locale = 'en')\n",
    "        human_readable = ' '.join(date.lower().split())\n",
    "        machine_readable = dt.isoformat()\n",
    "\n",
    "    except:\n",
    "        print()\n",
    "        raise RuntimeError\n",
    "\n",
    "    return [human_readable, machine_readable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(num): \n",
    "    return [random_date() for i in range(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_num_data = 40000\n",
    "num_train = int(tot_num_data * 0.7)\n",
    "num_val = int(tot_num_data * 0.1)\n",
    "num_test = int(tot_num_data * 0.2)\n",
    "\n",
    "original_train = create_dataset(num_train)\n",
    "original_val = create_dataset(num_val)\n",
    "original_test = create_dataset(num_test)\n",
    "preprocessor = DTPreprocessor()\n",
    "cleaned_train = preprocessor.cleanse_corpus(original_train)\n",
    "cleaned_val = preprocessor.cleanse_corpus(original_val)\n",
    "cleaned_test = preprocessor.cleanse_corpus(original_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14/01/2003</td>\n",
       "      <td>2003-01-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13 february, 2017</td>\n",
       "      <td>2017-02-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14/06/1984</td>\n",
       "      <td>1984-06-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1992, may 23</td>\n",
       "      <td>1992-05-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999, 22 october</td>\n",
       "      <td>1999-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>oct 15, 1974</td>\n",
       "      <td>1974-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1987/05/16</td>\n",
       "      <td>1987-05-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26/02/1983</td>\n",
       "      <td>1983-02-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17 december, 1980</td>\n",
       "      <td>1980-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18/01/1976</td>\n",
       "      <td>1976-01-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0           1\n",
       "0         14/01/2003  2003-01-14\n",
       "1  13 february, 2017  2017-02-13\n",
       "2         14/06/1984  1984-06-14\n",
       "3       1992, may 23  1992-05-23\n",
       "4   1999, 22 october  1999-10-22\n",
       "5       oct 15, 1974  1974-10-15\n",
       "6         1987/05/16  1987-05-16\n",
       "7         26/02/1983  1983-02-26\n",
       "8  17 december, 1980  1980-12-17\n",
       "9         18/01/1976  1976-01-18"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, building vocabulary and preparing the sequences to train the seq2seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the preprocessor on the cleansed corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit_on_corpus(cleaned_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sorted vocab for human:\n",
      "[('1', 40749), ('0', 34503), ('2', 30359), ('9', 27236), ('7', 11583), ('8', 11421), ('e', 10886), ('a', 9342), ('r', 8496), ('3', 7529), ('u', 6865), ('5', 6347), ('6', 6189), ('4', 6031), ('m', 5534), ('j', 4625), ('n', 4622), ('b', 4533), ('c', 3969), ('o', 3925), ('y', 3919), ('p', 3166), ('t', 3148), ('s', 2363), ('l', 2321), ('d', 1567), ('v', 1539), ('g', 1537), ('f', 1402), ('h', 792), ('i', 779)]\n",
      "****************************************************************************************************\n",
      "The sorted vocab for machine:\n",
      "[('0', 54826), ('1', 48560), ('2', 33341), ('9', 28838), ('7', 13134), ('8', 12955), ('3', 9100), ('5', 7950), ('6', 7707), ('4', 7589)]\n",
      "####################################################################################################\n",
      "The word-to-index map for human:\n",
      "PAD: 0\n",
      "SOS: 1\n",
      "EOS: 2\n",
      "UNK: 3\n",
      "1: 4\n",
      "0: 5\n",
      "2: 6\n",
      "9: 7\n",
      "7: 8\n",
      "8: 9\n",
      "e: 10\n",
      "a: 11\n",
      "r: 12\n",
      "3: 13\n",
      "u: 14\n",
      "5: 15\n",
      "6: 16\n",
      "4: 17\n",
      "m: 18\n",
      "j: 19\n",
      "n: 20\n",
      "b: 21\n",
      "c: 22\n",
      "o: 23\n",
      "y: 24\n",
      "p: 25\n",
      "t: 26\n",
      "s: 27\n",
      "l: 28\n",
      "d: 29\n",
      "v: 30\n",
      "g: 31\n",
      "f: 32\n",
      "h: 33\n",
      "i: 34\n",
      "****************************************************************************************************\n",
      "The word-to-index map for machine:\n",
      "PAD: 0\n",
      "SOS: 1\n",
      "EOS: 2\n",
      "-: 3\n",
      "0: 4\n",
      "1: 5\n",
      "2: 6\n",
      "9: 7\n",
      "7: 8\n",
      "8: 9\n",
      "3: 10\n",
      "5: 11\n",
      "6: 12\n",
      "4: 13\n"
     ]
    }
   ],
   "source": [
    "print(f\"The sorted vocab for human:\\n{preprocessor.human_sorted_vocab}\")\n",
    "print(\"*\"*100)\n",
    "print(f\"The sorted vocab for machine:\\n{preprocessor.machine_sorted_vocab}\")\n",
    "print(\"#\"*100)\n",
    "print(\"The word-to-index map for human:\")\n",
    "for word, idx in list(preprocessor.human_char2idx.items()):\n",
    "    print(f\"{word}: {idx}\")\n",
    "print(\"*\"*100)\n",
    "print(\"The word-to-index map for machine:\")\n",
    "for word, idx in list(preprocessor.machine_char2idx.items()):\n",
    "    print(f\"{word}: {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "SOS_token_idx = preprocessor.machine_char2idx['SOS']\n",
    "print(SOS_token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder, which is made up of ```num_layers``` layers of bidirectional LSTM cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers = 1, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # emb_dim is the input_dim of the LSTM\n",
    "        input_dim = emb_dim\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedder = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)   \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, dropout = dropout, bidirectional = True, batch_first = True)   \n",
    "\n",
    "        # We need one fully connected layer to map the \n",
    "        # 2*hidden_dim-dimensional bidirectional output to hidden_dim dimensions\n",
    "        self.fc_hidden = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.fc_cell = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, seqs):\n",
    "        # seqs shape: (batch_size, seq_length)\n",
    "        embedded = self.embedder(seqs) \n",
    "        # embedded shape: (batch_size, seq_length, input_dim) \n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # bidirectional = True, so\n",
    "        # outputs shape: (batch_size, seq_length, 2 * hidden_dim)\n",
    "        # hidden shape: (2 * num_layers, batch_size, hidden_dim)\n",
    "        # cell shape: (2 * num_layers, batch_size, hidden_dim)\n",
    "\n",
    "        # Firstly, only take the fisrt layer. After concatenating the forward and backward final states:\n",
    "        # hidden shape: (1, batch_size, 2 * hidden_dim)\n",
    "        # cell shape: (1, batch_size, 2 * hidden_dim)\n",
    "        output_hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim = 2))\n",
    "        output_cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim = 2))\n",
    "        # After the fc later:\n",
    "        # output_hidden shape: (1, batch_size, hidden_dim)\n",
    "        # output_cell shape: (1, batch_size, hidden_dim)\n",
    "        \n",
    "        # Then, take all the following layers if there is any\n",
    "        if self.num_layers > 1:\n",
    "            for idx in range(1, self.num_layers):\n",
    "                output_hidden = torch.cat((self.fc_hidden(torch.cat((hidden[2*idx:2*idx+1], hidden[2*idx+1:2*idx+2]), dim = 2)), output_hidden), dim = 0)\n",
    "                output_cell = torch.cat((self.fc_cell(torch.cat((cell[2*idx:2*idx+1], cell[2*idx+1:2*idx+2]), dim = 2)), output_cell), dim = 0)\n",
    "        # output_hidden shape: (num_layers, batch_size, hidden_dim)\n",
    "        # output_cell shape: (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "        return outputs, output_hidden, output_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder, which is made up of ```num_layers``` (the same number of layers as encoder) unidirectional LSTM cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, output_dim, num_layers = 1, dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # emb_dim is the input_dim of the LSTM\n",
    "        input_dim = emb_dim\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.embedder = nn.Embedding(vocab_size, emb_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(input_dim + 2 * hidden_dim, hidden_dim, num_layers, dropout = dropout, batch_first = True)\n",
    "        self.attention_layer = AttentionLayer(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, seqs, encoder_outputs, hidden, cell):\n",
    "        # seqs shape: (batch_size, 1)\n",
    "        embedded = self.embedder(seqs)\n",
    "        # embedded shape: (batch_size, 1, input_dim)\n",
    "        # encoder_outputs shape: (batch_size, en_seq_len, 2 * hidden_dim)\n",
    "        # hidden (cell) shape: (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "        # Only take the states from the last layer of the last time step \n",
    "        last_hidden = hidden[-1:, :, :]\n",
    "        # last_hidden shape: (1, batch_size, hidden_dim)\n",
    "        \n",
    "        context_vector, _ = self.attention_layer(encoder_outputs, last_hidden)\n",
    "        # context_vector shape: (batch_size, 1, 2 * hidden_dim)\n",
    "\n",
    "        embedded = torch.cat((embedded, context_vector), dim = 2)\n",
    "        # embedded shape: (batch_size, 1, input_dim + 2 * hidden_dim)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # outputs shape: (batch_size, 1, hidden_dim)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_dim)\n",
    "        # cell shape: (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "        # predictions shape: (batch_size, 1, output_dim)\n",
    "\n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention layer that computes the context vector at each recurrent step of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "            energies: e = relu(V * concat(values, query))\n",
    "            attentions: alpha = softmax(e)\n",
    "            context_vector: a = sum(alpha * values)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Build a fc layer to take the (i-1)th hidden state of decoder and jth hidden state from encoder\n",
    "        # and map them to a energy value\n",
    "        self.energy = nn.Linear(3 * hidden_dim, 1)\n",
    "        # Use a softmax layer to normalize the energies into probabilities\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim = 1)  \n",
    "\n",
    "    def forward(self, values, query):\n",
    "        \"\"\"\n",
    "                values (encoder_outputs): (batch_size, seq_length, 2 * hidden_dim)\n",
    "                query (de_last_step_hidden): (1, batch_size, hidden_dim) -> (batch_size, en_seq_len, hidden_dim)\n",
    "                energies: (batch_size, en_seq_len, 1)\n",
    "                attentions: (batch_size, en_seq_len, 1)\n",
    "                context_vector: (batch_size, 1, 2 * hidden_dim)\n",
    "        \"\"\"\n",
    "        en_seq_len = values.shape[1]\n",
    "        query_expanded = query.repeat(en_seq_len, 1, 1)\n",
    "        # query_expanded shape: (en_seq_len, batch_size, hidden_dim)\n",
    "        query_reshaped = query_expanded.transpose(0, 1)\n",
    "        # query_reshaped shape: (batch_size, en_seq_len, hidden_dim)\n",
    "\n",
    "        energies = self.relu(self.energy(torch.cat((query_reshaped, values), dim = 2)))\n",
    "        # energies shape: (batch_size, en_seq_len, 1)\n",
    "        attentions = self.softmax(energies)\n",
    "        # attentions shape: (batch_size, en_seq_len, 1)\n",
    "\n",
    "        context_vector = torch.bmm(attentions.permute(0, 2, 1), values)\n",
    "        # context_vector shape: (batch_size, 1, 2 * hidden_dim)\n",
    "\n",
    "        return context_vector, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 en_vocab_size,\n",
    "                 de_vocab_size,\n",
    "                 en_emb_dim,\n",
    "                 de_emb_dim,\n",
    "                 hidden_dim,\n",
    "                 num_layers = 1,\n",
    "                 dropout = 0.1,\n",
    "                 device = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.de_vocab_size = de_vocab_size\n",
    "        self.encoder = Encoder(en_vocab_size, en_emb_dim, hidden_dim, num_layers = num_layers, dropout = dropout)\n",
    "        self.decoder = Decoder(de_vocab_size, de_emb_dim, hidden_dim, de_vocab_size, num_layers = num_layers, dropout = dropout)\n",
    "\n",
    "    def update_device(self, device):\n",
    "        self.device = device\n",
    "        \n",
    "    def infer(self, source_seqs, max_target_seq_len, SOS_token_idx):\n",
    "        batch_size = source_seqs.shape[0]\n",
    "\n",
    "        # Initialize a tensor to store outputs. Remember to exclude the zero vector at the first step\n",
    "        outputs = torch.zeros(batch_size, 1, self.de_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(source_seqs)\n",
    "\n",
    "        # Use the SOS token idx as the initial input\n",
    "        x = torch.ones([batch_size, 1], dtype = source_seqs.dtype).to(self.device)\n",
    "        x = x * SOS_token_idx\n",
    "\n",
    "        for idx in range(0, max_target_seq_len):\n",
    "            output, hidden, cell = self.decoder(x, encoder_outputs, hidden, cell)\n",
    "            outputs = torch.cat((outputs, output), dim = 1)\n",
    "            # Using one-element slicing to preserve the dimension\n",
    "            x = output.argmax(dim = 2)\n",
    "        return outputs[:, 1:, :] # Exclude the zero vector at the first step\n",
    "\n",
    "    def forward(self, source_seqs, target_seqs, teacher_force_ratio = 0.5):  \n",
    "        batch_size, target_seq_len = target_seqs.shape\n",
    "\n",
    "        # Initialize a tensor to store outputs. Remember to exclude the zero vector at the first step\n",
    "        outputs = torch.zeros(batch_size, 1, self.de_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(source_seqs)\n",
    "\n",
    "        # Grab the SOS token idx as the initial input\n",
    "        x = target_seqs[:, 0:1]\n",
    "\n",
    "        for idx in range(1, target_seq_len):\n",
    "            output, hidden, cell = self.decoder(x, encoder_outputs, hidden, cell)\n",
    "            outputs = torch.cat((outputs, output), dim = 1)\n",
    "            # Using one-element slicing to preserve the dimension\n",
    "            x = target_seqs[:, idx:idx+1] if random.random() < teacher_force_ratio else output.argmax(dim = 2)\n",
    "        return outputs[:, 1:, :] # Exclude the zero vector at the first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2seq(10, 20, 8, 16, 32, num_layers = 2, device = device).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check for the inputs and outputs dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 14, 20])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randint(0, 10, (4, 10)).to(device)\n",
    "outputs = torch.randint(0, 10, (4, 15)).to(device)\n",
    "print(model(inputs, outputs).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# BUILDING MODEL ###################\n",
    "import time\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_collate(batch):\n",
    "    \n",
    "    (xx, yy) = zip(*batch)\n",
    "\n",
    "    xx_pad = preprocessor.pad_minibatch_collate(xx)\n",
    "    yy_pad = preprocessor.pad_minibatch_collate(yy)\n",
    "\n",
    "    return xx_pad, yy_pad\n",
    "\n",
    "def experiment(train_dataset, val_dataset, args):\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    model = Seq2seq(preprocessor.human_num_tokens, preprocessor.human_num_tokens, args.emb_dim, args.emb_dim, args.hidden_dim, num_layers = args.num_layers, dropout = args.dropout, device = device)\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = args.batch_size, shuffle = False, collate_fn = pad_collate)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = args.batch_size, shuffle = False, collate_fn = pad_collate)\n",
    "\n",
    "    # ****** Copy model to device ****** #\n",
    "    model.to(device)\n",
    "\n",
    "    # ====== Loss function ====== #\n",
    "    lossfunction = nn.CrossEntropyLoss(reduction = 'sum', ignore_index = 0) \n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "    # ====== Data collection ====== #\n",
    "    list_epoch = [] \n",
    "    list_train_loss = []\n",
    "    list_val_loss = []\n",
    "    list_train_acc = []\n",
    "    list_val_acc = []\n",
    "    \n",
    "    # #################### Calculate the begining losses #################### #\n",
    "    model.eval() # Set the model be 'evaluate mode' \n",
    "    \n",
    "    list_epoch.append(0)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        train_loss = 0 # to sum up each batch\n",
    "        train_acc = 0 # to sum up each batch\n",
    "        normalization = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_dataloader:\n",
    "            \n",
    "            # ****** Transfer data to device ****** #\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            pred_y = model(batch_X, batch_y)\n",
    "\n",
    "            loss = lossfunction(pred_y.contiguous().view(-1, pred_y.shape[-1]), batch_y[:, 1:].contiguous().view(-1))\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            normalization += (batch_y[:, 1:] != 0).sum().item()\n",
    "            \n",
    "            train_acc += torch.all((pred_y.argmax(dim = 2) == batch_y[:, 1:]), dim = 1).sum().item()\n",
    "            \n",
    "        train_loss = train_loss / normalization\n",
    "        train_acc = train_acc / num_train\n",
    "        list_train_loss.append(train_loss)\n",
    "        list_train_acc.append(train_acc)\n",
    "\n",
    "        val_loss = 0 # to sum up each batch\n",
    "        val_acc = 0 # to sum up each batch\n",
    "        normalization = 0\n",
    "        \n",
    "        for batch_X, batch_y in val_dataloader:\n",
    "\n",
    "            # ****** Transfer data to device ****** #\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            pred_y = model.infer(batch_X, batch_y.shape[1] - 1, SOS_token_idx)\n",
    "            \n",
    "            loss = lossfunction(pred_y.contiguous().view(-1, pred_y.shape[-1]), batch_y[:, 1:].contiguous().view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            normalization += (batch_y[:, 1:] != 0).sum().item()\n",
    "            \n",
    "            val_acc += torch.all((pred_y.argmax(dim = 2) == batch_y[:, 1:]), dim = 1).sum().item()\n",
    "\n",
    "        val_loss = val_loss / normalization\n",
    "        val_acc = val_acc / num_val\n",
    "        list_val_loss.append(val_loss)\n",
    "        list_val_acc.append(val_acc)\n",
    "    \n",
    "    best_val_acc = val_acc\n",
    "    stored_train_acc = train_acc\n",
    "    ################################################################################\n",
    "    \n",
    "    # ====== Loop ====== #\n",
    "    for epoch in range(1, args.epoch + 1):  \n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        # ====== Train ====== #\n",
    "        model.train() # Set the model be 'train mode' \n",
    "        \n",
    "        list_epoch.append(epoch)\n",
    "        \n",
    "        train_loss = 0 # to sum up each batch\n",
    "        train_acc = 0 # to sum up each batch\n",
    "        normalization = 0\n",
    "                \n",
    "        for batch_X, batch_y in train_dataloader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # ****** Transfer data to device ****** #\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            pred_y = model(batch_X, batch_y)\n",
    "            \n",
    "            loss = lossfunction(pred_y.contiguous().view(-1, pred_y.shape[-1]), batch_y[:, 1:].contiguous().view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            normalization += (batch_y[:, 1:] != 0).sum().item()\n",
    "            \n",
    "            train_acc += torch.all((pred_y.argmax(dim = 2) == batch_y[:, 1:]), dim = 1).sum().item()\n",
    "                                                \n",
    "        train_loss = train_loss / normalization\n",
    "        train_acc = train_acc / num_train\n",
    "        list_train_loss.append(train_loss)\n",
    "        list_train_acc.append(train_acc)\n",
    "        \n",
    "        print(f'{time.time() - t0} seconds')\n",
    "\n",
    "        # ====== Validation ====== #\n",
    "        model.eval() # Set the model be 'evaluate mode' \n",
    "        \n",
    "        val_loss = 0 # to sum up each batch\n",
    "        val_acc = 0 # to sum up each batch\n",
    "        normalization = 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            for batch_X, batch_y in val_dataloader:\n",
    "        \n",
    "                # ****** Transfer data to device ****** #\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                pred_y = model.infer(batch_X, batch_y.shape[1] - 1, SOS_token_idx)\n",
    "                \n",
    "                loss = lossfunction(pred_y.contiguous().view(-1, pred_y.shape[-1]), batch_y[:, 1:].contiguous().view(-1))\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                normalization += (batch_y[:, 1:] != 0).sum().item()\n",
    "            \n",
    "                val_acc += torch.all((pred_y.argmax(dim = 2) == batch_y[:, 1:]), dim = 1).sum().item()\n",
    "                \n",
    "        val_loss = val_loss / normalization\n",
    "        val_acc = val_acc / num_val\n",
    "        list_val_loss.append(val_loss)\n",
    "        list_val_acc.append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            print(f\"Epoch {epoch}: The best validation acc increased from {best_val_acc: .4f} to {val_acc: .4f}\")\n",
    "            best_val_acc = val_acc\n",
    "            stored_train_acc = train_acc\n",
    "            torch.save(model.state_dict(), './model/model_dict_best.pt')\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}: The best validation acc did not increase\")\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Train Loss: {train_loss: .4f}, Val Loss: {val_loss: .4f}, Train acc: {train_acc: .4f}, Val Acc: {val_acc: .4f}')\n",
    "        print('~' * 100)\n",
    "        \n",
    "        if args.es:\n",
    "            if val_acc < best_val_acc:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 0\n",
    "            if count >= args.patience:\n",
    "                break\n",
    "                \n",
    "    print(f\"Best validation acc: {best_val_acc}\")\n",
    "    print(f\"Corresponding train acc: {stored_train_acc}\")\n",
    "        \n",
    "    return list_epoch, list_train_loss, list_val_loss, list_train_acc, list_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedder): Embedding(35, 32)\n",
      "    (dropout): Dropout(p=0.15, inplace=False)\n",
      "    (lstm): LSTM(32, 64, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "    (fc_hidden): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc_cell): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedder): Embedding(35, 32)\n",
      "    (dropout): Dropout(p=0.15, inplace=False)\n",
      "    (lstm): LSTM(160, 64, num_layers=2, batch_first=True, dropout=0.15)\n",
      "    (attention_layer): AttentionLayer(\n",
      "      (energy): Linear(in_features=192, out_features=1, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (softmax): Softmax(dim=1)\n",
      "    )\n",
      "    (fc): Linear(in_features=64, out_features=35, bias=True)\n",
      "  )\n",
      ")\n",
      "20.45079517364502 seconds\n",
      "Epoch 1: The best validation acc did not increase\n",
      "Epoch: 1, Train Loss:  1.8320, Val Loss:  1.4459, Train acc:  0.0000, Val Acc:  0.0000\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "20.416141033172607 seconds\n",
      "Epoch 2: The best validation acc increased from  0.0000 to  0.0013\n",
      "Epoch: 2, Train Loss:  1.0992, Val Loss:  0.8856, Train acc:  0.0003, Val Acc:  0.0013\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "20.553817749023438 seconds\n",
      "Epoch 3: The best validation acc increased from  0.0013 to  0.0055\n",
      "Epoch: 3, Train Loss:  0.7864, Val Loss:  0.7016, Train acc:  0.0030, Val Acc:  0.0055\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "20.83083987236023 seconds\n",
      "Epoch 4: The best validation acc increased from  0.0055 to  0.0330\n",
      "Epoch: 4, Train Loss:  0.6177, Val Loss:  0.5566, Train acc:  0.0183, Val Acc:  0.0330\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.028351068496704 seconds\n",
      "Epoch 5: The best validation acc increased from  0.0330 to  0.1298\n",
      "Epoch: 5, Train Loss:  0.4734, Val Loss:  0.4167, Train acc:  0.0702, Val Acc:  0.1298\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.398632764816284 seconds\n",
      "Epoch 6: The best validation acc increased from  0.1298 to  0.3937\n",
      "Epoch: 6, Train Loss:  0.3411, Val Loss:  0.2769, Train acc:  0.2266, Val Acc:  0.3937\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "20.94499921798706 seconds\n",
      "Epoch 7: The best validation acc increased from  0.3937 to  0.6830\n",
      "Epoch: 7, Train Loss:  0.2239, Val Loss:  0.1644, Train acc:  0.4809, Val Acc:  0.6830\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "20.878093957901 seconds\n",
      "Epoch 8: The best validation acc increased from  0.6830 to  0.8500\n",
      "Epoch: 8, Train Loss:  0.1419, Val Loss:  0.0962, Train acc:  0.7130, Val Acc:  0.8500\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "20.898664951324463 seconds\n",
      "Epoch 9: The best validation acc increased from  0.8500 to  0.9363\n",
      "Epoch: 9, Train Loss:  0.0872, Val Loss:  0.0644, Train acc:  0.8561, Val Acc:  0.9363\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "20.935019969940186 seconds\n",
      "Epoch 10: The best validation acc increased from  0.9363 to  0.9812\n",
      "Epoch: 10, Train Loss:  0.0547, Val Loss:  0.0332, Train acc:  0.9290, Val Acc:  0.9812\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "20.948150873184204 seconds\n",
      "Epoch 11: The best validation acc increased from  0.9812 to  0.9928\n",
      "Epoch: 11, Train Loss:  0.0378, Val Loss:  0.0211, Train acc:  0.9563, Val Acc:  0.9928\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "20.945373058319092 seconds\n",
      "Epoch 12: The best validation acc did not increase\n",
      "Epoch: 12, Train Loss:  0.0263, Val Loss:  0.0155, Train acc:  0.9766, Val Acc:  0.9925\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.018550157546997 seconds\n",
      "Epoch 13: The best validation acc increased from  0.9928 to  0.9952\n",
      "Epoch: 13, Train Loss:  0.0206, Val Loss:  0.0115, Train acc:  0.9808, Val Acc:  0.9952\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.084458112716675 seconds\n",
      "Epoch 14: The best validation acc did not increase\n",
      "Epoch: 14, Train Loss:  0.0158, Val Loss:  0.0094, Train acc:  0.9865, Val Acc:  0.9948\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.043699979782104 seconds\n",
      "Epoch 15: The best validation acc did not increase\n",
      "Epoch: 15, Train Loss:  0.0125, Val Loss:  0.0079, Train acc:  0.9890, Val Acc:  0.9950\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.088736057281494 seconds\n",
      "Epoch 16: The best validation acc did not increase\n",
      "Epoch: 16, Train Loss:  0.0105, Val Loss:  0.0074, Train acc:  0.9908, Val Acc:  0.9948\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.02914810180664 seconds\n",
      "Epoch 17: The best validation acc did not increase\n",
      "Epoch: 17, Train Loss:  0.0094, Val Loss:  0.0063, Train acc:  0.9905, Val Acc:  0.9952\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.1091091632843 seconds\n",
      "Epoch 18: The best validation acc did not increase\n",
      "Epoch: 18, Train Loss:  0.0087, Val Loss:  0.0064, Train acc:  0.9911, Val Acc:  0.9950\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.08183789253235 seconds\n",
      "Epoch 19: The best validation acc increased from  0.9952 to  0.9955\n",
      "Epoch: 19, Train Loss:  0.0082, Val Loss:  0.0063, Train acc:  0.9901, Val Acc:  0.9955\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.06717801094055 seconds\n",
      "Epoch 20: The best validation acc did not increase\n",
      "Epoch: 20, Train Loss:  0.0063, Val Loss:  0.0060, Train acc:  0.9935, Val Acc:  0.9945\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.143998861312866 seconds\n",
      "Epoch 21: The best validation acc increased from  0.9955 to  0.9958\n",
      "Epoch: 21, Train Loss:  0.0055, Val Loss:  0.0047, Train acc:  0.9943, Val Acc:  0.9958\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.08996295928955 seconds\n",
      "Epoch 22: The best validation acc increased from  0.9958 to  0.9960\n",
      "Epoch: 22, Train Loss:  0.0052, Val Loss:  0.0047, Train acc:  0.9938, Val Acc:  0.9960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.351346015930176 seconds\n",
      "Epoch 23: The best validation acc did not increase\n",
      "Epoch: 23, Train Loss:  0.0066, Val Loss:  0.0051, Train acc:  0.9900, Val Acc:  0.9952\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.153573989868164 seconds\n",
      "Epoch 24: The best validation acc did not increase\n",
      "Epoch: 24, Train Loss:  0.0042, Val Loss:  0.0041, Train acc:  0.9947, Val Acc:  0.9955\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.215006828308105 seconds\n",
      "Epoch 25: The best validation acc did not increase\n",
      "Epoch: 25, Train Loss:  0.0039, Val Loss:  0.0040, Train acc:  0.9944, Val Acc:  0.9960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.3281409740448 seconds\n",
      "Epoch 26: The best validation acc did not increase\n",
      "Epoch: 26, Train Loss:  0.0051, Val Loss:  0.0044, Train acc:  0.9916, Val Acc:  0.9932\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.372654914855957 seconds\n",
      "Epoch 27: The best validation acc increased from  0.9960 to  0.9965\n",
      "Epoch: 27, Train Loss:  0.0044, Val Loss:  0.0032, Train acc:  0.9932, Val Acc:  0.9965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.180345058441162 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: The best validation acc did not increase\n",
      "Epoch: 28, Train Loss:  0.0031, Val Loss:  0.0031, Train acc:  0.9951, Val Acc:  0.9962\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.25106906890869 seconds\n",
      "Epoch 29: The best validation acc did not increase\n",
      "Epoch: 29, Train Loss:  0.0043, Val Loss:  0.0036, Train acc:  0.9925, Val Acc:  0.9950\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.285172939300537 seconds\n",
      "Epoch 30: The best validation acc did not increase\n",
      "Epoch: 30, Train Loss:  0.0030, Val Loss:  0.0047, Train acc:  0.9953, Val Acc:  0.9942\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.218317985534668 seconds\n",
      "Epoch 31: The best validation acc did not increase\n",
      "Epoch: 31, Train Loss:  0.0042, Val Loss:  0.0034, Train acc:  0.9918, Val Acc:  0.9950\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.304983139038086 seconds\n",
      "Epoch 32: The best validation acc did not increase\n",
      "Epoch: 32, Train Loss:  0.0036, Val Loss:  0.0028, Train acc:  0.9934, Val Acc:  0.9960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.2668399810791 seconds\n",
      "Epoch 33: The best validation acc did not increase\n",
      "Epoch: 33, Train Loss:  0.0018, Val Loss:  0.0027, Train acc:  0.9966, Val Acc:  0.9965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.31337022781372 seconds\n",
      "Epoch 34: The best validation acc did not increase\n",
      "Epoch: 34, Train Loss:  0.0027, Val Loss:  0.0036, Train acc:  0.9943, Val Acc:  0.9945\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.340636253356934 seconds\n",
      "Epoch 35: The best validation acc increased from  0.9965 to  0.9968\n",
      "Epoch: 35, Train Loss:  0.0019, Val Loss:  0.0023, Train acc:  0.9964, Val Acc:  0.9968\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.294650316238403 seconds\n",
      "Epoch 36: The best validation acc did not increase\n",
      "Epoch: 36, Train Loss:  0.0018, Val Loss:  0.0023, Train acc:  0.9965, Val Acc:  0.9962\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.306922912597656 seconds\n",
      "Epoch 37: The best validation acc did not increase\n",
      "Epoch: 37, Train Loss:  0.0022, Val Loss:  0.0026, Train acc:  0.9952, Val Acc:  0.9958\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.274035930633545 seconds\n",
      "Epoch 38: The best validation acc did not increase\n",
      "Epoch: 38, Train Loss:  0.0019, Val Loss:  0.0024, Train acc:  0.9958, Val Acc:  0.9960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.41214108467102 seconds\n",
      "Epoch 39: The best validation acc did not increase\n",
      "Epoch: 39, Train Loss:  0.0015, Val Loss:  0.0022, Train acc:  0.9970, Val Acc:  0.9962\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.27889609336853 seconds\n",
      "Epoch 40: The best validation acc did not increase\n",
      "Epoch: 40, Train Loss:  0.0014, Val Loss:  0.0028, Train acc:  0.9971, Val Acc:  0.9950\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.310891151428223 seconds\n",
      "Epoch 41: The best validation acc did not increase\n",
      "Epoch: 41, Train Loss:  0.0028, Val Loss:  0.0038, Train acc:  0.9932, Val Acc:  0.9945\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.39351773262024 seconds\n",
      "Epoch 42: The best validation acc increased from  0.9968 to  0.9970\n",
      "Epoch: 42, Train Loss:  0.0016, Val Loss:  0.0023, Train acc:  0.9965, Val Acc:  0.9970\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.323381185531616 seconds\n",
      "Epoch 43: The best validation acc did not increase\n",
      "Epoch: 43, Train Loss:  0.0009, Val Loss:  0.0022, Train acc:  0.9978, Val Acc:  0.9965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.36111807823181 seconds\n",
      "Epoch 44: The best validation acc did not increase\n",
      "Epoch: 44, Train Loss:  0.0007, Val Loss:  0.0020, Train acc:  0.9985, Val Acc:  0.9968\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.387526988983154 seconds\n",
      "Epoch 45: The best validation acc did not increase\n",
      "Epoch: 45, Train Loss:  0.0007, Val Loss:  0.0024, Train acc:  0.9982, Val Acc:  0.9955\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.316141843795776 seconds\n",
      "Epoch 46: The best validation acc did not increase\n",
      "Epoch: 46, Train Loss:  0.0006, Val Loss:  0.0024, Train acc:  0.9989, Val Acc:  0.9958\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.343104124069214 seconds\n",
      "Epoch 47: The best validation acc did not increase\n",
      "Epoch: 47, Train Loss:  0.0039, Val Loss:  0.0039, Train acc:  0.9889, Val Acc:  0.9935\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.37074112892151 seconds\n",
      "Epoch 48: The best validation acc did not increase\n",
      "Epoch: 48, Train Loss:  0.0016, Val Loss:  0.0051, Train acc:  0.9962, Val Acc:  0.9930\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.33298420906067 seconds\n",
      "Epoch 49: The best validation acc did not increase\n",
      "Epoch: 49, Train Loss:  0.0011, Val Loss:  0.0025, Train acc:  0.9976, Val Acc:  0.9952\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.325495958328247 seconds\n",
      "Epoch 50: The best validation acc did not increase\n",
      "Epoch: 50, Train Loss:  0.0007, Val Loss:  0.0023, Train acc:  0.9985, Val Acc:  0.9965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.582366943359375 seconds\n",
      "Epoch 51: The best validation acc did not increase\n",
      "Epoch: 51, Train Loss:  0.0020, Val Loss:  0.0026, Train acc:  0.9949, Val Acc:  0.9960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.552714109420776 seconds\n",
      "Epoch 52: The best validation acc did not increase\n",
      "Epoch: 52, Train Loss:  0.0008, Val Loss:  0.0021, Train acc:  0.9979, Val Acc:  0.9962\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.507619857788086 seconds\n",
      "Epoch 53: The best validation acc did not increase\n",
      "Epoch: 53, Train Loss:  0.0009, Val Loss:  0.0029, Train acc:  0.9975, Val Acc:  0.9940\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.32563591003418 seconds\n",
      "Epoch 54: The best validation acc did not increase\n",
      "Epoch: 54, Train Loss:  0.0006, Val Loss:  0.0022, Train acc:  0.9983, Val Acc:  0.9958\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.432846069335938 seconds\n",
      "Epoch 55: The best validation acc did not increase\n",
      "Epoch: 55, Train Loss:  0.0004, Val Loss:  0.0021, Train acc:  0.9990, Val Acc:  0.9958\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.350116968154907 seconds\n",
      "Epoch 56: The best validation acc did not increase\n",
      "Epoch: 56, Train Loss:  0.0005, Val Loss:  0.0025, Train acc:  0.9986, Val Acc:  0.9955\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.417650938034058 seconds\n",
      "Epoch 57: The best validation acc did not increase\n",
      "Epoch: 57, Train Loss:  0.0011, Val Loss:  0.0053, Train acc:  0.9976, Val Acc:  0.9835\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.395652770996094 seconds\n",
      "Epoch 58: The best validation acc did not increase\n",
      "Epoch: 58, Train Loss:  0.0029, Val Loss:  0.0021, Train acc:  0.9923, Val Acc:  0.9958\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.45793914794922 seconds\n",
      "Epoch 59: The best validation acc did not increase\n",
      "Epoch: 59, Train Loss:  0.0009, Val Loss:  0.0024, Train acc:  0.9982, Val Acc:  0.9968\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.402162313461304 seconds\n",
      "Epoch 60: The best validation acc did not increase\n",
      "Epoch: 60, Train Loss:  0.0005, Val Loss:  0.0021, Train acc:  0.9986, Val Acc:  0.9958\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.379904747009277 seconds\n",
      "Epoch 61: The best validation acc did not increase\n",
      "Epoch: 61, Train Loss:  0.0004, Val Loss:  0.0021, Train acc:  0.9990, Val Acc:  0.9965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.44414210319519 seconds\n",
      "Epoch 62: The best validation acc did not increase\n",
      "Epoch: 62, Train Loss:  0.0004, Val Loss:  0.0024, Train acc:  0.9989, Val Acc:  0.9962\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.39497399330139 seconds\n",
      "Epoch 63: The best validation acc did not increase\n",
      "Epoch: 63, Train Loss:  0.0003, Val Loss:  0.0024, Train acc:  0.9989, Val Acc:  0.9960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.554133892059326 seconds\n",
      "Epoch 64: The best validation acc did not increase\n",
      "Epoch: 64, Train Loss:  0.0003, Val Loss:  0.0020, Train acc:  0.9991, Val Acc:  0.9965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.380412817001343 seconds\n",
      "Epoch 65: The best validation acc did not increase\n",
      "Epoch: 65, Train Loss:  0.0003, Val Loss:  0.0022, Train acc:  0.9990, Val Acc:  0.9948\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.41097903251648 seconds\n",
      "Epoch 66: The best validation acc did not increase\n",
      "Epoch: 66, Train Loss:  0.0005, Val Loss:  0.0029, Train acc:  0.9985, Val Acc:  0.9940\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.418641328811646 seconds\n",
      "Epoch 67: The best validation acc did not increase\n",
      "Epoch: 67, Train Loss:  0.0025, Val Loss:  0.0026, Train acc:  0.9928, Val Acc:  0.9952\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.405299186706543 seconds\n",
      "Epoch 68: The best validation acc increased from  0.9970 to  0.9972\n",
      "Epoch: 68, Train Loss:  0.0011, Val Loss:  0.0022, Train acc:  0.9970, Val Acc:  0.9972\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.390307188034058 seconds\n",
      "Epoch 69: The best validation acc did not increase\n",
      "Epoch: 69, Train Loss:  0.0005, Val Loss:  0.0022, Train acc:  0.9983, Val Acc:  0.9960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.430968761444092 seconds\n",
      "Epoch 70: The best validation acc did not increase\n",
      "Epoch: 70, Train Loss:  0.0004, Val Loss:  0.0022, Train acc:  0.9990, Val Acc:  0.9968\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.41737985610962 seconds\n",
      "Epoch 71: The best validation acc did not increase\n",
      "Epoch: 71, Train Loss:  0.0002, Val Loss:  0.0021, Train acc:  0.9995, Val Acc:  0.9968\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.382612943649292 seconds\n",
      "Epoch 72: The best validation acc did not increase\n",
      "Epoch: 72, Train Loss:  0.0002, Val Loss:  0.0021, Train acc:  0.9995, Val Acc:  0.9965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.489269256591797 seconds\n",
      "Epoch 73: The best validation acc did not increase\n",
      "Epoch: 73, Train Loss:  0.0015, Val Loss:  0.0143, Train acc:  0.9967, Val Acc:  0.9765\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.437384843826294 seconds\n",
      "Epoch 74: The best validation acc did not increase\n",
      "Epoch: 74, Train Loss:  0.0027, Val Loss:  0.0026, Train acc:  0.9924, Val Acc:  0.9952\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.3542959690094 seconds\n",
      "Epoch 75: The best validation acc did not increase\n",
      "Epoch: 75, Train Loss:  0.0010, Val Loss:  0.0023, Train acc:  0.9978, Val Acc:  0.9968\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.390825033187866 seconds\n",
      "Epoch 76: The best validation acc did not increase\n",
      "Epoch: 76, Train Loss:  0.0004, Val Loss:  0.0023, Train acc:  0.9988, Val Acc:  0.9960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.50147008895874 seconds\n",
      "Epoch 77: The best validation acc did not increase\n",
      "Epoch: 77, Train Loss:  0.0002, Val Loss:  0.0026, Train acc:  0.9993, Val Acc:  0.9962\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.478607177734375 seconds\n",
      "Epoch 78: The best validation acc did not increase\n",
      "Epoch: 78, Train Loss:  0.0011, Val Loss:  0.0061, Train acc:  0.9968, Val Acc:  0.9858\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.4989173412323 seconds\n",
      "Epoch 79: The best validation acc did not increase\n",
      "Epoch: 79, Train Loss:  0.0006, Val Loss:  0.0027, Train acc:  0.9985, Val Acc:  0.9960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.415440797805786 seconds\n",
      "Epoch 80: The best validation acc did not increase\n",
      "Epoch: 80, Train Loss:  0.0002, Val Loss:  0.0026, Train acc:  0.9994, Val Acc:  0.9955\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.666653871536255 seconds\n",
      "Epoch 81: The best validation acc did not increase\n",
      "Epoch: 81, Train Loss:  0.0001, Val Loss:  0.0027, Train acc:  0.9995, Val Acc:  0.9965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.48106575012207 seconds\n",
      "Epoch 82: The best validation acc did not increase\n",
      "Epoch: 82, Train Loss:  0.0001, Val Loss:  0.0024, Train acc:  0.9996, Val Acc:  0.9965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.45797109603882 seconds\n",
      "Epoch 83: The best validation acc did not increase\n",
      "Epoch: 83, Train Loss:  0.0001, Val Loss:  0.0024, Train acc:  0.9997, Val Acc:  0.9968\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.446521759033203 seconds\n",
      "Epoch 84: The best validation acc did not increase\n",
      "Epoch: 84, Train Loss:  0.0001, Val Loss:  0.0026, Train acc:  0.9996, Val Acc:  0.9968\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.464004039764404 seconds\n",
      "Epoch 85: The best validation acc did not increase\n",
      "Epoch: 85, Train Loss:  0.0001, Val Loss:  0.0028, Train acc:  0.9997, Val Acc:  0.9960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.525191068649292 seconds\n",
      "Epoch 86: The best validation acc did not increase\n",
      "Epoch: 86, Train Loss:  0.0002, Val Loss:  0.0037, Train acc:  0.9993, Val Acc:  0.9950\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.473556995391846 seconds\n",
      "Epoch 87: The best validation acc did not increase\n",
      "Epoch: 87, Train Loss:  0.0028, Val Loss:  0.0029, Train acc:  0.9918, Val Acc:  0.9948\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.449578046798706 seconds\n",
      "Epoch 88: The best validation acc did not increase\n",
      "Epoch: 88, Train Loss:  0.0005, Val Loss:  0.0024, Train acc:  0.9987, Val Acc:  0.9960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.512171030044556 seconds\n",
      "Epoch 89: The best validation acc did not increase\n",
      "Epoch: 89, Train Loss:  0.0005, Val Loss:  0.0022, Train acc:  0.9984, Val Acc:  0.9965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.52029013633728 seconds\n",
      "Epoch 90: The best validation acc did not increase\n",
      "Epoch: 90, Train Loss:  0.0003, Val Loss:  0.0022, Train acc:  0.9990, Val Acc:  0.9952\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.49670934677124 seconds\n",
      "Epoch 91: The best validation acc did not increase\n",
      "Epoch: 91, Train Loss:  0.0003, Val Loss:  0.0024, Train acc:  0.9990, Val Acc:  0.9965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.545154094696045 seconds\n",
      "Epoch 92: The best validation acc did not increase\n",
      "Epoch: 92, Train Loss:  0.0002, Val Loss:  0.0021, Train acc:  0.9992, Val Acc:  0.9970\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.503670930862427 seconds\n",
      "Epoch 93: The best validation acc did not increase\n",
      "Epoch: 93, Train Loss:  0.0002, Val Loss:  0.0023, Train acc:  0.9994, Val Acc:  0.9965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.503089904785156 seconds\n",
      "Epoch 94: The best validation acc did not increase\n",
      "Epoch: 94, Train Loss:  0.0002, Val Loss:  0.0024, Train acc:  0.9994, Val Acc:  0.9968\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.50989270210266 seconds\n",
      "Epoch 95: The best validation acc did not increase\n",
      "Epoch: 95, Train Loss:  0.0001, Val Loss:  0.0026, Train acc:  0.9995, Val Acc:  0.9955\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.500229120254517 seconds\n",
      "Epoch 96: The best validation acc did not increase\n",
      "Epoch: 96, Train Loss:  0.0012, Val Loss:  0.0041, Train acc:  0.9966, Val Acc:  0.9932\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.528746128082275 seconds\n",
      "Epoch 97: The best validation acc did not increase\n",
      "Epoch: 97, Train Loss:  0.0008, Val Loss:  0.0024, Train acc:  0.9975, Val Acc:  0.9958\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.516917943954468 seconds\n",
      "Epoch 98: The best validation acc did not increase\n",
      "Epoch: 98, Train Loss:  0.0004, Val Loss:  0.0028, Train acc:  0.9986, Val Acc:  0.9962\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.542492866516113 seconds\n",
      "Epoch 99: The best validation acc did not increase\n",
      "Epoch: 99, Train Loss:  0.0007, Val Loss:  0.0037, Train acc:  0.9979, Val Acc:  0.9958\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "21.48640489578247 seconds\n",
      "Epoch 100: The best validation acc did not increase\n",
      "Epoch: 100, Train Loss:  0.0005, Val Loss:  0.0029, Train acc:  0.9983, Val Acc:  0.9960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Best validation acc: 0.99725\n",
      "Corresponding train acc: 0.997\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.epoch = 100\n",
    "args.batch_size = 128\n",
    "args.emb_dim = 32\n",
    "args.hidden_dim = 64\n",
    "args.num_layers = 2\n",
    "args.dropout = 0.15\n",
    "args.patience = 20\n",
    "args.es = False\n",
    "\n",
    "train_dataset = DTDataset(preprocessor.human_sequences, preprocessor.machine_sequences)\n",
    "\n",
    "val_human_sequences, val_machine_sequences = preprocessor.get_sequences_of_test_texts(cleaned_val)\n",
    "val_dataset = DTDataset(val_human_sequences, val_machine_sequences)\n",
    "\n",
    "list_epoch, list_train_loss, list_val_loss, list_train_acc, list_val_acc = experiment(train_dataset, val_dataset, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "# ====== Plot Loss ====== #\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(list_epoch, list_train_loss, label='train_loss')\n",
    "ax1.plot(list_epoch, list_val_loss, '--', label='val_loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "ax1.set_title('epoch vs loss')\n",
    "\n",
    "# ====== Plot Metric ====== #\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(list_epoch, list_train_acc, marker='x', label='train_acc')\n",
    "ax2.plot(list_epoch, list_val_acc, marker='x', label='val_acc')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "ax2.set_title('epoch vs acc')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2seq(preprocessor.human_num_tokens, preprocessor.human_num_tokens, args.emb_dim, args.emb_dim, args.hidden_dim, num_layers = args.num_layers, dropout = args.dropout, device = device)\n",
    "model.load_state_dict(torch.load('./model/model_dict_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set:  99.60%\n"
     ]
    }
   ],
   "source": [
    "test_human_sequences, test_machine_sequences = preprocessor.get_sequences_of_test_texts(cleaned_test)\n",
    "test_dataset = DTDataset(test_human_sequences, test_machine_sequences)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = args.batch_size, shuffle = False, collate_fn = pad_collate)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.eval() # Set the model be 'evaluate mode' \n",
    "    \n",
    "with torch.inference_mode():\n",
    "\n",
    "    test_acc = 0 # to sum up each batch\n",
    "\n",
    "    for batch_X, batch_y in test_dataloader:\n",
    "\n",
    "        # ****** Transfer data to device ****** #\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        pred_y = model.infer(batch_X, batch_y.shape[1] - 1, SOS_token_idx)\n",
    "\n",
    "        test_acc += torch.all((pred_y.argmax(dim = 2) == batch_y[:, 1:]), dim = 1).sum().item()\n",
    "\n",
    "    test_acc = test_acc / num_test\n",
    "\n",
    "print(f\"Accuracy on the test set: {test_acc * 100: .2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(input_seq, model, SOS_idx, max_seq_len):\n",
    "    '''\n",
    "            This function is used to generate an output sequence self-recursively according to \n",
    "        an input sequence.\n",
    "    '''\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    input_seq_formatted = torch.tensor([input_seq], dtype = torch.int64).to(device)\n",
    "\n",
    "    output_seq = model.infer(input_seq_formatted, max_seq_len, SOS_token_idx)\n",
    "    \n",
    "    return output_seq.argmax(dim = 2)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, see how the model predicts the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[1 4 UNK 0 1 UNK 2 0 0 3]\n",
      "True: 2003-01-14\n",
      "Pred: 2003-01-14\n",
      "\n",
      "\n",
      "[1 3 UNK f e b r u a r y UNK 2 0 1 7]\n",
      "True: 2017-02-13\n",
      "Pred: 2017-02-13\n",
      "\n",
      "\n",
      "[1 4 UNK 0 6 UNK 1 9 8 4]\n",
      "True: 1984-06-14\n",
      "Pred: 1984-06-14\n",
      "\n",
      "\n",
      "[1 9 9 2 UNK m a y UNK 2 3]\n",
      "True: 1992-05-23\n",
      "Pred: 1992-05-23\n",
      "\n",
      "\n",
      "[1 9 9 9 UNK 2 2 UNK o c t o b e r]\n",
      "True: 1999-10-22\n",
      "Pred: 1999-10-22\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "\n",
    "    input_seq = preprocessor.human_sequences[i]\n",
    "\n",
    "    output_seq = predict_sequence(input_seq, model, 1, 10)\n",
    "    \n",
    "    output_seq = output_seq.cpu().detach().numpy().tolist()\n",
    "    \n",
    "    output_seq_formatted = []\n",
    "    for ele in output_seq:\n",
    "        if ele != 2:\n",
    "            output_seq_formatted.append(ele)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    print(f\"[{' '.join([preprocessor.human_idx2char[token] for token in preprocessor.human_sequences[i][1:-1]])}]\")\n",
    "    print(f\"True: {''.join([preprocessor.machine_idx2char[token] for token in preprocessor.machine_sequences[i][1:-1]])}\")\n",
    "    print(f\"Pred: {''.join([preprocessor.machine_idx2char[token] for token in output_seq_formatted])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make up some dates and try the model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H: 2014,05,06\n",
      "[2 0 1 4 UNK 0 5 UNK 0 6]\n",
      "M: 2014-05-06\n",
      "\n",
      "\n",
      "H: 2022/12/21\n",
      "[2 0 2 2 UNK 1 2 UNK 2 1]\n",
      "M: 2022-12-21\n",
      "\n",
      "\n",
      "H: 19 April 1993\n",
      "[1 9 UNK a p r i l UNK 1 9 9 3]\n",
      "M: 1993-04-19\n",
      "\n",
      "\n",
      "H: May, 01, 2000\n",
      "[m a y UNK 0 1 UNK 2 0 0 0]\n",
      "M: 2000-05-01\n",
      "\n",
      "\n",
      "H: 24,08,2018\n",
      "[2 4 UNK 0 8 UNK 2 0 1 8]\n",
      "M: 2018-08-24\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentences = [\n",
    "                   [\"2014,05,06\"], \n",
    "                   [\"2022/12/21\"], \n",
    "                   [\"19 April 1993\"],\n",
    "                   ['May, 01, 2000'],\n",
    "                   ['24,08,2018']\n",
    "                  ]\n",
    "\n",
    "input_cleaned = preprocessor.cleanse_corpus(input_sentences)\n",
    "\n",
    "input_seqs = preprocessor.get_sequences_of_inference_texts(input_cleaned)\n",
    "\n",
    "for i in range(len(input_seqs)):\n",
    "    input_seq = input_seqs[i]\n",
    "    \n",
    "    output_seq = predict_sequence(input_seq, model, 1, 10)\n",
    "    \n",
    "    output_seq = output_seq.cpu().detach().numpy().tolist()\n",
    "    \n",
    "    output_seq_formatted = []\n",
    "    for ele in output_seq:\n",
    "        if ele != 2:\n",
    "            output_seq_formatted.append(ele)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    words = []\n",
    "    for idx in output_seq_formatted:\n",
    "        words.append(preprocessor.machine_idx2char[idx])\n",
    "    output_sentence = \"\".join(words)\n",
    "\n",
    "    print(f\"H: {input_sentences[i][0]}\")\n",
    "    print(f\"[{' '.join([preprocessor.human_idx2char[token] for token in input_seq[1:-1]])}]\")\n",
    "    print(f\"M: {output_sentence}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad eh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMsr5cT4nGEybYESEmFAYWl",
   "collapsed_sections": [],
   "name": "BBox Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
